######################################TO BE CONFIRMED###################################################
19. What is same antecendents were retrieved for different consequent (if there are more than 2 consequents)?

<Geoff>Discussing why it might happen and the reason behind it (distribution).

20. Differences between SDRD and 
Ma, B. L. W. H. Y., & Liu, B. (1998, August). Integrating classification and association rule mining. 
In Proceedings of the fourth international conference on knowledge discovery and data mining. 

<Geoff> Classification is for Preditive 

21. Only one type of consequent will be retrieved from WEKA Apriori - Classification Association Rule.

22. Why in the alpha calculation, the power of 2 needs to minus 1? Global.cpp, line 47

23. When inserting the candidate self-sufficient itemset into memory, the only criteria is whether current K exceeds but do not check if
the minimum value in memory is greater or smaller than current itemset.

#########################################################################################
Questions of original OpusMiner:
#########################################################################################
1. Command -p was not indicated in usageStr and what does command -p do? : in opus_miner.cpp, line 80 

The p command was the flag to perform Bonferroni Correction in OpusMiner or not.
------------------------------------------------------------------------------------------------------------------------------------

2. What does it mean by search by Lift in command -l : in opus_miner.cpp, line 111

There are two measurements proposed to calculate the upper bound value of an item: Leverage/Lift; The command -l aims to set the flag to use which measurements.
------------------------------------------------------------------------------------------------------------------------------------

3. Why the p value needs to calculate with log factorial? isn't it only factorial would be fine? Try to scale the data to lower range? : in fisher.cpp, line 51

With large sample data, the pure factorial will be really large and somehow may exceeds the range of (unsigned) int. However, by calculating the log factorial 
and then round with e to the power of invariant result, will keep the meaning of p value from Fisher Exact Test and also scale down the double value of p.
------------------------------------------------------------------------------------------------------------------------------------ 

4. Why there's an array for alpha? in globals.h, line 45

It originates to the idea of Bonfferoni Correction which divide the significance level alfa with sample size. However, it's more related to the concept of 
Layered critical values, which the assigned critical value to evaluate current itemsets is calculated according to searching depth but not total sample size.
------------------------------------------------------------------------------------------------------------------------------------

5. What is the null hypotheses to accept a single item? in find_itemsets.cpp, line 342

While evaluating the interestingness of a single item in fisher exact test, can view as the contingency table with the support count of current item in 
true negative cell.
------------------------------------------------------------------------------------------------------------------------------------ 

6. Why apriori is true when redundancy is true? find_itemsets.cpp, line 135, 298

The apriori variable here is nothing related to the Algorithm Apriori but the definition of anti-monotonicity property of an items and its subset. 
By filtering out one itemset, none of its superset needed to be scanned.
------------------------------------------------------------------------------------------------------------------------------------

7. Why need to check if upper bound value is greater than minValue? find_itemsets.cpp, line 287

When assessing the interestingness of an itemset, not only that the pvalue should be less than the layered significance level alpha 
(Reject H0 and Acccept H1 in such case), 
but also need to be greater than the minimum value of the top K because when top K itemsets are already existed, it's useless to include more itemsets 
whose upper bound value is less than the Kth itemset's upper bound value (current itemset will filter out anyway).

leverage = sup(s) - sup(A) * sup(s-A) <= sup(s) - sup(s) * sup(s)
s is the superset of A.

new : sup (antecedent + consequent) - sup (antecedent) * sup (consequent)
Only use one combination
------------------------------------------------------------------------------------------------------------------------------------

8. Why the count of immediate subset is equal to current itemset will mark it as redundant? find_itemsets.cpp, line 139
It's for the non-redundancy property in self-sufficient itemset. When there's subsumed items lying in an itemset, one of its subset and subset of subset 
will have the same support count. E.g. {female, pregnant} and {pregnant}.

#########################################################################################
**The key amendment to OpusMiner is that, when counting the support for an itemset, it will include the consequent; 
However, when checking the immediate or partition subsets of an itemset, it should exclude the consequent when looping every subsets, 
but when calculating the support count, it should include the consequent again.

A B Y
subsets: A, B, AY, BY, AB should all saved (TIDCount) in the memory.

1. Should I only save those itemsets: {1-itemset + consequent} who passes the Fisher Exact Test for correspondent {1-itemset}?
Yes. Only check those with consequent who pass the Fisher Exact Test, but have to save the support(TIDCount) of all subsets.

2. For example, I have {A}, {B}, {C} that without consequent Y to pass the FET, but only {AY}, {BY} pass FET, should I opt out {C}?
No. There's two criteria that a "good" itemset should be fulfilled. One is should exceeds the productive test (leverage/lift), 
one is the fisher exact test (Whether its supersets might be good).  When an itemset is redundant, it will not put into memory. 

If an itemset's any subset is identified as redundant (cannot find in TIDCount), this itemset is redundant.

**I. When calculating the upper bound value (leverage/lift), only need to consider the itemset that contain consequent;
**II. When calculating the lower bound value (p value in FET), only need to consider the itemset that contain consequent;
**III. When checking subsets of an itemset, all of its subsets should be non-redundant (Can be found in TIDCount)

3. When I check {AY, BY, CY,,,} should I still use lattice depth as 2 or should increase 1 every time for itemsets in lattice?
<Xuelin> Still follow the same depth in original lattice when adding Consequent.

4. In fisher exact test, Is the position for A and B matter? say, antecedent/consequent?
         |    !A   |     A    |
---------|---------|----------|
   !B    |   a     | b        |
---------|---------|----------|
   B     |   c     | d(count) | Count 2
---------|---------|----------|----------
         |         | Count 1  | Total transaction
 it does not matter.
 
 When checking {A, B}, the fisher exact test assigns {count = AB.tid.size, count1 = max(A.tid.size, B.tid.size), count2 = AB.tid.size.
 As such, c will always equal 0 in RXC table and makes the hypotheses that B exists in all transactions where A also exists (the extreme case that 
 B is subsumed with A like A = female, B = pregnant).
 
5. There's no special handling for different attributes have same values, e.g. in Mushroom data, there are three 
different attributes have "SMOOTH" values.
--Add identifiers to the different item names to make it unique with each other

6. Why the calculation of lift and leverage is different than usual in opus function? find_itemsets.cpp, line 283 
Because the lift/leverage calculation before pruning should be the upper bound values that an itemset may satisfy, and the lift/leverage calculation
in checksubsets function is the actual lift/leverage values that should save and print in the final output.

7. What does the meaning of closure?

Closed frequent itemsets.

8. p-value in "search by leverage", and actual lift in "search by lift" 

The reason for similar lift value in the top K itemsets is due to the same support between the superset and subset (exclude consequent)
Hence, these itemsets' lift value is equal to 1/sup(Consequent).

10. redundant itemsets will also retrieved according to the logic in opus()
<Xuelin> By checking if current itemset's redundancy (whether one of its subset has same Tidset), does not aim to identify current itemset is redundant.

For example, though {female} includes all transactions of {pregnant}, itemset of {female, pregnant} is not redundant. However, its superset will be redundant, as
any itemset {female, pregnant, X} will be redundant as {female} is useless.

Hence, the redundancy test here means the superset of current itemset will be redundant.

11. Numerical attributes? Cannot simple remove them as they are also in forming the interesting rules

	<Xuelin> Define the range of numerical attributes could be suitable way to discretize the feature. 

    Equal attributes? In adult dataset, attributes like "Education = Bachelors" and "Education year = 13" etc. they will form
    similar rules
    
    <Xuelin> This could be something to research and experiment further, but not something to focus at the moment.
    
12. From the result of "search by lift", itemsets are with small support but strong strength - sup(rule) = sup(ant)

	<Xuelin> That's the way lift will take effect on.
	
13. The self-sufficient itemsets who form the rules and meet the "necessary and sufficient" principle.
	E.g. 
	In Output_Leverage, ">50K/field9 = Male/field5 = Married-civ-spouse" (failed test for self sufficiency) are strong rule 
	that most people who earned more than 50K are the husband. However, that doesn't mean that all husbands can earn more than 50K.
	However, ">50K/field9 = Male/field5 = Married-civ-spouse/field2 = Private" or 
	">50K/field4 = EduNum13/field8 = White/field9 = Male/field5 = Married-civ-spouse" (both pass test)
	are "necessary and sufficient" principle to say, most of people who fulfil this rule have higher probability to earn more than 50K.
	
	Same theory can apply in Output_Lift, ">50K & field1 = Age41-50 & field4 = EduNum15 & field2 = Self-emp-inc" (failed the test)
	and ">50K & field1 = Age41-50 & field6 = Exec-managerial & field4 = EduNum15 & field2 = Self-emp-inc" (pass the test).
	
	<Xuelin> Yup, this could be the story, but should also explain mathematically why the itemset is not "interesting". 
	(Use exclusive domain calculation to explain)

14. There's bug in calculating the total number of transactions? load_data.cpp, line 49
Change the logic to make total number of transactions correct no matter there's empty line at the end of the file.

15. With layered critical value or without, the rules retrieved is different, how to explain this phenomenon?
<Xuelin> Compared with Bonferroni test where alfa in each hypothesis test will be divided by the total number of tests,
the layered critical value strategy will have lower probability to accept the itemsets in deeper levels within the lattice searching
space, and it's a trade-off between the larger search space (more rules will be found).

One should understand that with lower alfa value, slightly higher p-value of the null hypothesis will exceed this current alfa,
which results in "Fail to reject" the null hypothesis (Reject alternative hypothesis).

In statistical hypothesis testing, Type-I error happens due to reject the null hypothesis in spite of it’s true, 
which means the accepted patterns are likely to happen occasionally and numerous spurious patterns will be found by algorithms. 

In contrast, Type-II error occurs when failing to reject the null hypothesis while alternative hypothesis is true. 
In such case, the true productive patterns will be discarded.

For example, there are {a,b,c,d,e} four items in the searching space.
In level 1, we have {a}, {b}, {c}, {d}, {e}, five 1-itemsets
In level 2, we have {ab}, {ac}, {ad}, {ae}, {bc}, {bd}, {be}, {cd}, {ce}, {de} ten 2-itemsets
In level 3, we have {abc}, {abd}, {abe}, {acd}, {ace}, {ade}, {bcd}, {bce}, {cde} nine 3-itemsets
In level 4, we have {abcd}, {abce}, {acde}, three 4-itemsets
In level 5, we have {abcde}, one 1-itemset

With Bonferroni test, all alfa in the hypothesis tests will be, alfa' = alfa / (5 + 10 + 9 + 3 + 1) = alfa / 28
With Layered critical value approach:
In first level, alfa' = alfa / (2^1 * 5) = alfa / 10
In second level, alfa' = alfa / (2^2 * 10) = alfa / 40
In third level, alfa' = alfa / (2^3 * 9) = alfa / 72
In fourth level, alfa' = alfa / (2^4 * 3) = alfa / 48
In fith level, alfa' = alfa / (2^5 * 1) = alfa / 32
which means, it will have smaller alfa value compared with Bonferroni test, and the FISHER EXACT TEST will have higher probability to "Fail to reject" the 
null hypothesis (disregard current itemset). It's a trade-off while in larger datasets (more itemsets found).

16. Why the FISHER EXACT TEST in filter itemsets equal to the meaning of "Independent productive" test in 
Self-sufficient itemset principle?

The FISHER EXACT TEST gives the hypothesis that two exclusive subsets of current itemset, which also excludes
the exclusive domain (Alternative hypothesis: of all the exclusive items in productive/non-redundant supersets, the rest of the excluded
transactions might also make current itemset independent productive)

17. Should I add a comparison section in the final thesis to compare Classification and Supervised Descriptive Rule Discovery?
<Geoff> You should mention the difference between SDRD and classification, but it only needs a paragraph, not a section.

18. There's no same itemsets found between OpusMiner and BigML when search by lift. The support/confidence could not change
in BigML
<Geoff>I think that comparing speed with BigML is worth doing.  It is fine that it is only approximate as they are running on AWS and so have different compute power.  Just note all this in the thesis.
 
With search by lift fine to note that it is not possible to get equivalent results.

#########################################################################################
replace leverage/lift with mutual information
Before OpusMiner: identify which variable is consequent (get rid of it or use flag to identify it), perform only one type of consequent domain (e.g poison first)

When pruning itemsets, use mutual information instead of leverage or lift
Make the mutual information run without the upper bound.

#########################################################################################
Literature Review:
Association discovery (Apriori only, Not too much portion on other algorithms) (1/4)
Self-sufficient itemsets, k-optimal, Opus (5/12)
Supervised Descriptive Rule Discovery (1/3)

“A fuzzy association rule-based classification model for high-dimensional problems with genetic rule selection and lateral tuning”
“Fuzzy association rule mining algorithm for fast and efficient performance on very large datasets”

“Multi objective association rule mining with genetic algorithm without specifying minimum support and minimum confidence”

“The strategy of mining association rule based on cloud computing”

“Discovering associations with numeric variables” – Geoff Webb

1.	Sequential patterns
2.	Subgraph patterns
Infrequent patterns
------------------------------------------------------------

1. All of the subsets of the current itemsets should be stored and when checking in the next level, 
it do not need to check the support count of the correspondent subsets again.

itemID --> int
TID --> long
p_value --> double


tids: list of Tidset, the index is the item id
	{TidSet: list of TID, the transaction ids}


Itemset: List of 


Disjunction: OR, 并集
Conjunction: AND, 交集
#########################################################################################
Globals.itemNames: List<String>, List of item names
Globals.tids: List<Tidset>, Transaction list(Tidset) of each item with indexed of item id.
Globals.consequentTids: Tidset, the transaction list of consequent.
Globals.noOfTransactions: Total number of transactions, int
Globals.noOfItems: Total number of items, int

OpusMiner.itemsets: PriorityQueue<ItemsetRec>, the top k itemsets that identified as self-sufficient
Find_Itemsets.TIDCount: Map<Itemset, Integer>, the cover count of an itemset

1. Read documents
2. Check if first-level item can pass the Fisher Exact Test. 
	Need to join consequent at this moment?
		If yes, then need to join both X and consequent
		If no, then just remain the same
3. Save all productive 1-itemset to ItemQClass q


[-c] [-f] [-k <k>] [-l] [-p] [-r] [-s <consequent>] <input file> <output file>
-c -f -k 2 -l -p -r -s Y Test.txt Output.txt





